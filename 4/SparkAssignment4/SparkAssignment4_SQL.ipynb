{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "13505c2e",
      "metadata": {
        "id": "13505c2e"
      },
      "source": [
        "# Assignment 4 \n",
        "by Himani Anil Deshpande"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "70aa21f7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70aa21f7",
        "outputId": "6d778608-c1b3-412f-a90c-15d1b481a7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▋                              | 10 kB 13.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████                           | 30 kB 5.8 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 40 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 51 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 81 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 92 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 198 kB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: py4j\n",
            "Successfully installed py4j-0.10.9.3\n"
          ]
        }
      ],
      "source": [
        "pip install py4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry-n3F9aj53R",
        "outputId": "39f4d2e6-6560-4193-8742-3861ab6cb774"
      },
      "id": "ry-n3F9aj53R",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 37 kB/s \n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=b609ff2596f776804ca6439644627e45e7bd365cfb07084982ae1b31568e24de\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eec62cec",
      "metadata": {
        "id": "eec62cec"
      },
      "source": [
        "Connecting to Spark Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c7265b0f",
      "metadata": {
        "id": "c7265b0f"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType, IntegerType, DateType\n",
        "from pyspark.sql.functions import col, asc,desc\n",
        "import json\n",
        "import csv\n",
        "from io import StringIO\n",
        "from pyspark.sql import HiveContext\n",
        "sc = SparkContext()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af1cd8d8",
      "metadata": {
        "id": "af1cd8d8"
      },
      "source": [
        "Load fake_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "249b40b6",
      "metadata": {
        "id": "249b40b6"
      },
      "outputs": [],
      "source": [
        "\n",
        "# datafile = \"file:///Users/himanideshpande/miniforge3/envs/ENV_SPARK/SparkInstallation/spark-3.2.1-bin-hadoop2.7/IPYNB/Fake_data.csv\"\n",
        "datafile = \"/content/Fake_data.csv\"\n",
        "# input_json = sc.textFile(datafile)\n",
        "def loadRecord(line):\n",
        "    \"\"\"Parse a CSV line\"\"\"\n",
        "    input = StringIO(line)\n",
        "    reader = csv.DictReader(input, fieldnames=[\"index\",\"Birth_Country\",\"Email\",\"First_Name\",\"Income\",\"Job\",\"Last_name\",\"Loan_Approved\",\"SSN\"])\n",
        "    return next(reader)\n",
        "\n",
        "input_data = sc.textFile(datafile).map(loadRecord)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "22a407a3",
      "metadata": {
        "id": "22a407a3"
      },
      "outputs": [],
      "source": [
        "rdd = sc.parallelize(input_data.collect())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rdd.collect()\n",
        "\n",
        "spark = SparkSession(sc)"
      ],
      "metadata": {
        "id": "xhmj5xJtnoeX"
      },
      "id": "xhmj5xJtnoeX",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =  rdd.toDF()"
      ],
      "metadata": {
        "id": "-MM_cjlym-2X"
      },
      "id": "-MM_cjlym-2X",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTjDoRBiq6p6",
        "outputId": "01d8485d-705e-4420-8b0a-b0c9a3d01321"
      },
      "id": "CTjDoRBiq6p6",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Birth_Country: string (nullable = true)\n",
            " |-- Email: string (nullable = true)\n",
            " |-- First_Name: string (nullable = true)\n",
            " |-- Income: string (nullable = true)\n",
            " |-- Job: string (nullable = true)\n",
            " |-- Last_name: string (nullable = true)\n",
            " |-- Loan_Approved: string (nullable = true)\n",
            " |-- SSN: string (nullable = true)\n",
            " |-- index: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('Income', col('Income').cast(IntegerType()))"
      ],
      "metadata": {
        "id": "5V9H04Pmr81J"
      },
      "id": "5V9H04Pmr81J",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYhcjuoYsOqi",
        "outputId": "24cf6a47-3fd4-4594-df5c-8e1255bcb79c"
      },
      "id": "MYhcjuoYsOqi",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Birth_Country: string (nullable = true)\n",
            " |-- Email: string (nullable = true)\n",
            " |-- First_Name: string (nullable = true)\n",
            " |-- Income: integer (nullable = true)\n",
            " |-- Job: string (nullable = true)\n",
            " |-- Last_name: string (nullable = true)\n",
            " |-- Loan_Approved: string (nullable = true)\n",
            " |-- SSN: string (nullable = true)\n",
            " |-- index: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "9080492e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9080492e",
        "outputId": "75c6273a-9302-49ae-f806-f7c2604f54a0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.rdd.RDD"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "type(rdd)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4bd7d25",
      "metadata": {
        "id": "a4bd7d25"
      },
      "source": [
        "Question 1. \n",
        "Find birth country which has highest amount of people "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"Fake_data\")\n",
        "\n",
        "sqlDF = spark.sql(\"SELECT COunt(Birth_Country), Birth_Country FROM Fake_data group By Birth_Country order by 1 DESC Limit 1\")\n",
        "sqlDF.show()\n"
      ],
      "metadata": {
        "id": "CSk_tAr9rS9J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aef702d-60a0-4bef-c671-4bc81676a2fc"
      },
      "id": "CSk_tAr9rS9J",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------+\n",
            "|count(Birth_Country)|Birth_Country|\n",
            "+--------------------+-------------+\n",
            "|                  91|        Korea|\n",
            "+--------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "671889c7",
      "metadata": {
        "id": "671889c7"
      },
      "source": [
        "Question 2\n",
        "Find average income of people who are born in united states of america "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sqlDF = spark.sql(\"SELECT Avg(tb.Income), tb.Birth_Country FROM Fake_data tb  group by tb.Birth_Country having tb.Birth_Country = 'United States of America' \")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQbZfvr9P2Fi",
        "outputId": "e5b1e771-8a7c-4895-c864-8f5359883ba9"
      },
      "id": "uQbZfvr9P2Fi",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+--------------------+\n",
            "|       avg(Income)|       Birth_Country|\n",
            "+------------------+--------------------+\n",
            "|208759.82352941178|United States of ...|\n",
            "+------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 How many people has income over 100,000 but their loan is not approved. "
      ],
      "metadata": {
        "id": "jjvOfEDHsbAo"
      },
      "id": "jjvOfEDHsbAo"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sqlDF = spark.sql(\"SELECT COUNT(*) FROM Fake_data tb where tb.Income > 100000 and tb.Loan_Approved = False \")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhFto79huEpn",
        "outputId": "d0cf09e0-599b-45b8-a169-90f4951641db"
      },
      "id": "AhFto79huEpn",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|    4009|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 Find top 10 people with highest income in United States of america. (Print their names, income and jobs)"
      ],
      "metadata": {
        "id": "zbagZCdPuL_4"
      },
      "id": "zbagZCdPuL_4"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sqlDF = spark.sql(\"SELECT tb.First_Name,tb.Last_name, tb.Job, tb.Income  FROM Fake_data tb where tb.Birth_Country = 'United States of America' order by tb.Income DESC Limit 10 \")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAqU8P3pRGJb",
        "outputId": "f0eaf825-b314-435b-e471-4fe3dc141f61"
      },
      "id": "HAqU8P3pRGJb",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+---------+--------------------+------+\n",
            "|First_Name|Last_name|                 Job|Income|\n",
            "+----------+---------+--------------------+------+\n",
            "|    Alyssa|   Miller|Amenity horticult...|482588|\n",
            "|    Hunter|    Walls|Psychologist, pri...|468946|\n",
            "|      Rose|Henderson|Adult guidance wo...|426115|\n",
            "|  Danielle|  Leonard|Furniture conserv...|389810|\n",
            "|     Terry|    Klein|       Meteorologist|380410|\n",
            "|     Cindy|   Newton|Research scientis...|370322|\n",
            "|     Scott| Mitchell|       Art therapist|368913|\n",
            "|   Christy| Sandoval|      Engineer, land|355150|\n",
            "|     Kelly| Reynolds|           Press sub|341448|\n",
            "|  Kristina|    Smith|           Herbalist|338804|\n",
            "+----------+---------+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 How many number of distinct jobs are there?"
      ],
      "metadata": {
        "id": "hCYV1stTvnkU"
      },
      "id": "hCYV1stTvnkU"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sqlDF = spark.sql(\"SELECT count(DISTINCT(tb.Job)) FROM Fake_data tb  \")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4-NpgdAaR-uv",
        "outputId": "8d551f30-430b-4804-b0d4-9760cd9b0f29"
      },
      "id": "4-NpgdAaR-uv",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|count(DISTINCT Job)|\n",
            "+-------------------+\n",
            "|                640|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6 How many writers earn less than 100,000? "
      ],
      "metadata": {
        "id": "obTBXOKav9_f"
      },
      "id": "obTBXOKav9_f"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sqlDF = spark.sql(\"SELECT count(*) FROM Fake_data tb where tb.Income <100000 and tb.Job = 'Writer' \")\n",
        "sqlDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoUBeyo_SH6O",
        "outputId": "df8427c6-ea4c-4fb7-8522-9c1c81e93015"
      },
      "id": "XoUBeyo_SH6O",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(1)|\n",
            "+--------+\n",
            "|       5|\n",
            "+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference\n",
        "\n",
        "https://spark.apache.org/docs/2.2.0/sql-programming-guide.html#:~:text=Spark%20SQL%20is%20a%20Spark,information%20to%20perform%20extra%20optimizations."
      ],
      "metadata": {
        "id": "-q-zSBIdSxs9"
      },
      "id": "-q-zSBIdSxs9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "SQL SparkAssignment4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}