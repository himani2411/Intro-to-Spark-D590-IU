{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkAssignment5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intro To Spark Assignment 5\n",
        "submitted by Himani Anil Deshpande"
      ],
      "metadata": {
        "id": "e5am70UG2XyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Describe what are Accumulators and Broadcast Variables in Spark and when to use these shared variables?"
      ],
      "metadata": {
        "id": "ylsCP81E2i2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shared Variables:\n",
        "\n",
        "When a function passed to a Spark operation (such as a map or reduction) to run on a remote cluster node, it doesnt work on the original copy of the variables but works with a separate copy of all the variables used in the function. \n",
        "\n",
        "\n",
        "These copies of variables are available in all the remote nodes and while the operation runs on these nodes the updates are done on these copies of variables and not the original copy, but the results from these nodes doesnt return the updates done on these copies of variables to the driver program.\n",
        "\n",
        "\n",
        "\n",
        "Supporting common read / write shared variables between tasks is inefficient. But, Spark provides two restricted types of shared variables for two common usage patterns: broadcast variables and accumulators.\n"
      ],
      "metadata": {
        "id": "FVMmzjAy3GW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broadcast Variables:\n",
        "\n",
        "Spark uses efficient broadcast algorithms to distribute broadcast variables and keep the communication cost to the minimum. These variables are for keeping a read-only copy of the variables which are cached on each node rather than transferring the copy with the job/task. As they are read-only no updates need to be propogated.\n",
        "These are useful when we need to keep a copy of a large dataset on every node.\n",
        "The easiest way to meet read-only requirements is to pass a reference to a primitive value or immutable object. In such cases, you can only change the value of the broadcast variable in the driver code.\n",
        "\n",
        "Spark actions are performed through a series of stages separated by a distributed \"shuffle\" operation. Automatically, Spark sends the common data needed for each stage of the task. The data transferred in this way is cached in a serialized format and deserialized before  each task is performed. In other words, explicitly creating broadcast variables  only makes sense if the task requires the same data across multiple stages, or if it is important to cache the data in a deserialized format.\n",
        "\n",
        "\n",
        "Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. "
      ],
      "metadata": {
        "id": "0HT07ufd4Z36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accumulators:\n",
        "\n",
        "\n",
        "\n",
        "Accumulators provide a simple syntax for aggregating values from worker nodes back to the driver program\n",
        "\n",
        "Accumulators are variables that are \"added\" only by associative law and commutative operations, so they can  be efficiently supported in parallel. \n",
        "\n",
        "These can be used to implement counters, similar to MapReduce counters or sums. Spark by default supports numeric accumulators with added support for new types. \n",
        "we can create named or unnamed accumulators. A named accumulator  appears in the web UI of the stage."
      ],
      "metadata": {
        "id": "KcansVj0KKks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJSzLaHE2ykm",
        "outputId": "69212265-648d-4209-cdc4-2b0d423c1390"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting py4j\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[K     |████████████████████████████████| 199 kB 4.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: py4j\n",
            "Successfully installed py4j-0.10.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cQD5NN_RvLS",
        "outputId": "06e223e5-1078-4855-d4ac-07ce5121c868"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 34 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 49.4 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=c0ee23ee2784eecc9f5a4f13b8cc897530d960aac762e699f4c019f9eb34513d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "  Attempting uninstall: py4j\n",
            "    Found existing installation: py4j 0.10.9.5\n",
            "    Uninstalling py4j-0.10.9.5:\n",
            "      Successfully uninstalled py4j-0.10.9.5\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import DoubleType, IntegerType, DateType\n",
        "from pyspark.sql.functions import col, asc,desc\n",
        "import json\n",
        "import csv\n",
        "from io import StringIO\n",
        "from pyspark.sql import HiveContext\n",
        "sc = SparkContext()"
      ],
      "metadata": {
        "id": "xq0wTBvsRxbf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Examples of Accumulator"
      ],
      "metadata": {
        "id": "dFtMsD6tWuGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num = sc.accumulator(0) \n",
        "def func_add(x): \n",
        "   global num \n",
        "   num+=x \n",
        "  #  print(['*']*num)\n",
        "rdd = sc.parallelize([1,2,3,4,5]) \n",
        "rdd.foreach(func_add) \n",
        "final = num.value \n",
        "print (\"Accumulated value is ->\", final)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyY3Ui_tRzpZ",
        "outputId": "6f00a390-1958-484f-9e39-c76d893375c5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accumulated value is -> 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example the values of RDD are summed and stored in Accumulator"
      ],
      "metadata": {
        "id": "CtX40_XlWwmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "# import pyspark\n",
        "from functools import partial\n",
        "\n",
        "def remove_odd(item, accumulator):\n",
        "    if item % 2 == 0:\n",
        "        accumulator += 1\n",
        "    return '0' in str(item)\n",
        "\n",
        "\n",
        "accumulator = sc.accumulator(0)\n",
        "count_filter = partial(remove_odd, accumulator=accumulator)\n",
        "\n",
        "print(sc.range(0, 100).filter(count_filter).sum())\n",
        "\n",
        "print('accum', accumulator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECZ8Rz2TV1pQ",
        "outputId": "aac70218-ba58-43ad-d8c9-c0148a1c0d22"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "450\n",
            "accum 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above example we add up the accumulator only for even numbers from 1 to 100\n",
        "We use the functools.partial to create the counting filter, which remembers our accumulator variable and then print the sum and the final value of the accumulator"
      ],
      "metadata": {
        "id": "3aLtYVxQW41e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Broadcast Variable Example"
      ],
      "metadata": {
        "id": "6WBAKg0MZIj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from pyspark.sql import SQLContext\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "\n",
        "country = {\"IN\":\"India\", \"KR\":\"South Korea\", \"DK\":\"Denmark\", \"AR\":\"Argentina\"}\n",
        "broadcast_country = sc.broadcast(country)\n",
        "\n",
        "data = [(\"Shaun\",\"Verghese\",\"IN\"),\n",
        "    (\"Chandra\",\"Shah\",\"IN\"),\n",
        "    (\"Paula\",\"Williams\",\"DK\"),\n",
        "    (\"Maria\",\"D'costa\",\"AR\"),\n",
        "     (\"Kim\",\"Unn Park\",\"KR\")\n",
        "  ]\n",
        "\n",
        "columns = [\"firstname\",\"lastname\",\"country\"]\n",
        "df = sqlContext.createDataFrame(data = data, schema = columns)\n",
        "df.printSchema()\n",
        "df.show(truncate=False)\n",
        "\n",
        "def state_convert(code):\n",
        "    return broadcast_country.value[code]\n",
        "\n",
        "result = df.rdd.map(lambda x: (x[0],x[1],state_convert(x[2]))).toDF(columns)\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nxw-SClXEIb",
        "outputId": "b00c6dca-9a9b-4f4e-c372-ae1e36b65461"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pyspark/sql/context.py:79: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- firstname: string (nullable = true)\n",
            " |-- lastname: string (nullable = true)\n",
            " |-- country: string (nullable = true)\n",
            "\n",
            "+---------+--------+-------+\n",
            "|firstname|lastname|country|\n",
            "+---------+--------+-------+\n",
            "|Shaun    |Verghese|IN     |\n",
            "|Chandra  |Shah    |IN     |\n",
            "|Paula    |Williams|DK     |\n",
            "|Maria    |D'costa |AR     |\n",
            "|Kim      |Unn Park|KR     |\n",
            "+---------+--------+-------+\n",
            "\n",
            "+---------+--------+-----------+\n",
            "|firstname|lastname|country    |\n",
            "+---------+--------+-----------+\n",
            "|Shaun    |Verghese|India      |\n",
            "|Chandra  |Shah    |India      |\n",
            "|Paula    |Williams|Denmark    |\n",
            "|Maria    |D'costa |Argentina  |\n",
            "|Kim      |Unn Park|South Korea|\n",
            "+---------+--------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I the above example, I am creating a dataframe containing Person data and a braodcast variables containing the lookup for a few counties codes. \n",
        "Then I use the bradcast variable to convert these country codes(Alphabetical codes) to the Full country name."
      ],
      "metadata": {
        "id": "z_HG4YZZa4-l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "https://spark.apache.org/docs/2.2.0/rdd-programming-guide.html#shared-variables\n",
        "\n",
        "\n",
        "Canvas Modules"
      ],
      "metadata": {
        "id": "qqzyV60TTsnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Li0IvYK2c37R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}